{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI 4253 / 5253 - Lab #4 - Patent Problem with Spark RDD - SOLUTION\n",
    "<div>\n",
    " <h2> CSCI 4283 / 5253 \n",
    "  <IMG SRC=\"https://www.colorado.edu/cs/profiles/express/themes/cuspirit/logo.png\" WIDTH=50 ALIGN=\"right\"/> </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This [Spark cheatsheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf) is useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "import numpy as np\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf=SparkConf().setAppName(\"Lab4-rdd\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using PySpark and RDD's on the https://coding.csel.io machines is slow -- most of the code is executed in Python and this is much less efficient than the java-based code using the PySpark dataframes. Be patient and trying using `.cache()` to cache the output of joins. You may want to start with a reduced set of data before running the full task. You can use the `sample()` method to extract just a sample of the data or use "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two RDD's are called \"rawCitations\" and \"rawPatents\" because you probably want to process them futher (e.g. convert them to integer types, etc). \n",
    "\n",
    "The `textFile` function returns data in strings. This should work fine for this lab.\n",
    "\n",
    "Other methods you use might return data in type `Byte`. If you haven't used Python `Byte` types before, google it. You can convert a value of `x` type byte into e.g. a UTF8 string using `x.decode('uft-8')`. Alternatively, you can use the `open` method of the gzip library to read in all the lines as UTF-8 strings like this:\n",
    "```\n",
    "import gzip\n",
    "with gzip.open('cite75_99.txt.gz', 'rt',encoding='utf-8') as f:\n",
    "    rddCitations = sc.parallelize( f.readlines() )\n",
    "```\n",
    "This is less efficient than using `textFile` because `textFile` would use the underlying HDFS or other file system to read the file across all the worker nodes while the using `gzip.open()...readlines()` will read all the data in the frontend and then distribute it to all the worker nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddCitations = sc.textFile(\"cite75_99.txt.gz\")\n",
    "rddPatents = sc.textFile(\"apat63_99.txt.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data looks like the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"CITING\",\"CITED\"',\n",
       " '3858241,956203',\n",
       " '3858241,1324234',\n",
       " '3858241,3398406',\n",
       " '3858241,3557384']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddCitations.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"PATENT\",\"GYEAR\",\"GDATE\",\"APPYEAR\",\"COUNTRY\",\"POSTATE\",\"ASSIGNEE\",\"ASSCODE\",\"CLAIMS\",\"NCLASS\",\"CAT\",\"SUBCAT\",\"CMADE\",\"CRECEIVE\",\"RATIOCIT\",\"GENERAL\",\"ORIGINAL\",\"FWDAPLAG\",\"BCKGTLAG\",\"SELFCTUB\",\"SELFCTLB\",\"SECDUPBD\",\"SECDLWBD\"',\n",
       " '3070801,1963,1096,,\"BE\",\"\",,1,,269,6,69,,1,,0,,,,,,,',\n",
       " '3070802,1963,1096,,\"US\",\"TX\",,1,,2,6,63,,0,,,,,,,,,',\n",
       " '3070803,1963,1096,,\"US\",\"IL\",,1,,2,6,63,,9,,0.3704,,,,,,,',\n",
       " '3070804,1963,1096,,\"US\",\"OH\",,1,,2,6,63,,3,,0.6667,,,,,,,']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddPatents.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, they are a single string with multiple CSV's. You will need to convert these to (K,V) pairs, probably convert the keys to `int` and so on. You'll need to `filter` out the header string as well since there's no easy way to extract all the lines except the first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Initialize Spark Context and Load Raw Data Files\n",
    "-Start Spark session and SparkContext. Read the citation and patent data files as RDDs and cache them for reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Lab4-RDD\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CITATIONS_PATH = \"cite75_99.txt.gz\"\n",
    "PATENTS_PATH = \"apat63_99.txt.gz\"\n",
    "\n",
    "rddCitations = sc.textFile(CITATIONS_PATH).cache()\n",
    "rddPatents = sc.textFile(PATENTS_PATH).cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16522439\n",
      "2923923\n"
     ]
    }
   ],
   "source": [
    "print(rddCitations.count())\n",
    "print(rddPatents.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Parse Patent Header and Identify Column Indices\n",
    "\n",
    "-Extract the header from patents data to determine the indices of key columns: PATENT, COUNTRY, and POSTATE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "pat_header = rddPatents.first()\n",
    "hdr = next(csv.reader([pat_header]))\n",
    "\n",
    "i_PATENT = hdr.index(\"PATENT\")\n",
    "i_COUNTRY = hdr.index(\"COUNTRY\")\n",
    "i_POSTATE = hdr.index(\"POSTATE\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3:\n",
    "- Define a function to extract US patents with non-empty state information. Create an RDD mapping patent_id to state, filtering out invalid entries, and cache it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_us_state(line):\n",
    "    cols = next(csv.reader([line]))\n",
    "    patent_id = cols[i_PATENT]\n",
    "    country = (cols[i_COUNTRY] or \"\").upper()\n",
    "    state = (cols[i_POSTATE] or \"\").upper()\n",
    "    if country == \"US\" and state:\n",
    "        return (patent_id, state)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "patent_to_state = (\n",
    "    rddPatents\n",
    "      .filter(lambda line: line != pat_header)\n",
    "      .map(extract_us_state)\n",
    "      .filter(lambda x: x is not None)\n",
    "      .cache()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4:\n",
    "Define a function to parse citation lines into (citing, cited) pairs. Handles lines separated by comma or whitespace, filtering invalid entries and cache the resulting RDD.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_citation(line):\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        return None\n",
    "    parts = line.split(\",\") if \",\" in line else line.split()\n",
    "    if len(parts) < 2:\n",
    "        return None\n",
    "    citing = parts[0].strip()\n",
    "    cited  = parts[1].strip()\n",
    "    if not citing.isdigit() or not cited.isdigit():\n",
    "        return None\n",
    "    return (citing, cited)\n",
    "\n",
    "cit_pairs = (\n",
    "    rddCitations\n",
    "      .map(parse_citation)\n",
    "      .filter(lambda x: x is not None)\n",
    "      .cache()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: \n",
    "\n",
    "- Join citation pairs with patent states for citing patents.\n",
    "- Rearrange data keyed by cited patent.\n",
    "- Join again to attach cited patent states.\n",
    "- Filter citations where citing and cited patents have the same state.\n",
    "- Count the number of same-state citations per citing patent and cache the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join to get CITING_STATE\n",
    "citing_join = cit_pairs.join(patent_to_state)\n",
    "\n",
    "# Re-key by CITED\n",
    "by_cited = citing_join.map(lambda x: (x[1][0], (x[0], x[1][1])))\n",
    "\n",
    "# Join to get CITED_STATE\n",
    "both_states = by_cited.join(patent_to_state)\n",
    "\n",
    "# Count same-state citations\n",
    "same_state_counts = (\n",
    "    both_states\n",
    "      .filter(lambda x: x[1][0][1] == x[1][1])\n",
    "      .map(lambda x: (x[1][0][0], 1))\n",
    "      .reduceByKey(lambda a, b: a + b)\n",
    "      .cache()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "571919"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "same_state_counts.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('5959466', 125),\n",
       " ('5983822', 103),\n",
       " ('6008204', 100),\n",
       " ('5952345', 98),\n",
       " ('5958954', 96),\n",
       " ('5998655', 96),\n",
       " ('5936426', 94),\n",
       " ('5739256', 90),\n",
       " ('5951547', 90),\n",
       " ('5913855', 90)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10 = same_state_counts.takeOrdered(10, key=lambda x: -x[1])\n",
    "top10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5959466 125\n",
      "5983822 103\n",
      "6008204 100\n",
      "5952345 98\n",
      "5958954 96\n",
      "5998655 96\n",
      "5936426 94\n",
      "5739256 90\n",
      "5951547 90\n",
      "5913855 90\n"
     ]
    }
   ],
   "source": [
    "# Get top 10 patents by same-state count\n",
    "top10 = (\n",
    "    same_state_counts\n",
    "      .takeOrdered(10, key=lambda x: -x[1])\n",
    ")\n",
    "\n",
    "for patent, count in top10:\n",
    "    print(patent, count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
